import os
import json
import pandas as pd
import random
import logging
from pathlib import Path

# ======================== é…ç½® ========================
INPUT_DIR = "../output/data"
OUTPUT_DIR = "../output/images/augmented"
CSV_FILE = "data.csv"
QA_CLASSIFICATION_FILE = "qa_classification.json"
QA_EXTRACTION_FILE = "qa_extraction.json"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)

# å›ºå®šåˆ†ç±»é—®ç­”é…ç½®
FIXED_CLASSIFICATION_QUESTION = "<image>\nå›¾ç‰‡æ˜¯ä»€ä¹ˆç±»åˆ«"
FIXED_CLASSIFICATION_ANSWER = "ç¤¾ä¼šä¿é™©å‚ä¿è¯æ˜"


def load_csv_data(csv_path):
    """åŠ è½½CSVæ•°æ®"""
    try:
        df = pd.read_csv(csv_path, encoding='utf-8')
        logger.info(f"âœ… æˆåŠŸåŠ è½½CSVæ–‡ä»¶: {csv_path}, å…± {len(df)} è¡Œæ•°æ®")
        return df
    except Exception as e:
        logger.error(f"âŒ åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {e}")
        return None


def get_image_files(directory):
    """è·å–ç›®å½•ä¸­æ‰€æœ‰çš„å›¾ç‰‡æ–‡ä»¶"""
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff']
    image_files = []

    for root, dirs, files in os.walk(directory):
        for file in files:
            if any(file.lower().endswith(ext) for ext in image_extensions):
                image_files.append(file)

    logger.info(f"ğŸ“ æ‰¾åˆ° {len(image_files)} ä¸ªå›¾ç‰‡æ–‡ä»¶")
    return image_files


def generate_classification_qa(image_files):
    """ç”Ÿæˆåˆ†ç±»QAå¯¹"""
    qa_list = []

    for img_name in image_files:
        img_path = f"./images/{img_name}"

        qa_item = {
            "images": [img_path],
            "messages": [
                {
                    "role": "user",
                    "content": FIXED_CLASSIFICATION_QUESTION
                },
                {
                    "role": "assistant",
                    "content": FIXED_CLASSIFICATION_ANSWER
                }
            ]
        }
        qa_list.append(qa_item)

    return qa_list


def generate_questions_and_answers(row_data):
    """æ ¹æ®ä¸€è¡Œæ•°æ®ç”Ÿæˆ3ä¸ªé—®é¢˜å’Œç­”æ¡ˆ"""
    name = str(row_data.get('å§“å', ''))
    gender = str(row_data.get('æ€§åˆ«', ''))
    id_number = str(row_data.get('è¯ä»¶å·ç ', ''))
    id_type = str(row_data.get('è¯ä»¶ç±»å‹', ''))
    insurance_period = str(row_data.get('å‚ä¿èµ·æ­¢æ—¶é—´', ''))
    company = str(row_data.get('å•ä½', ''))
    yanglao = str(row_data.get('å…»è€', ''))
    gongshang = str(row_data.get('å·¥ä¼¤', ''))
    shiyue = str(row_data.get('å¤±ä¸š', ''))
    province = str(row_data.get('çœä»½å', ''))
    proof_date = str(row_data.get('è¯æ˜æ—¥æœŸ', ''))

    question_templates = [
        ("ç¼´çº³å…¬å¸æ˜¯ä»€ä¹ˆ", f"**{company}**"),
        ("è¯ä»¶ç±»å‹å’Œå·ç åˆ†åˆ«æ˜¯ä»€ä¹ˆ", f"{id_type} {id_number}"),
        ("å‚ä¿èµ·æ­¢æ—¶é—´æ˜¯ä»€ä¹ˆæ—¶å€™", f"**{insurance_period}**"),
        ("æ€§åˆ«æ˜¯ä»€ä¹ˆ", f"**{gender}**"),
        ("å…»è€ã€å·¥ä¼¤ã€å¤±ä¸šä¿é™©çŠ¶æ€", f"å…»è€ï¼š{yanglao}ï¼Œå·¥ä¼¤ï¼š{gongshang}ï¼Œå¤±ä¸šï¼š{shiyue}"),
        ("çœä»½æ˜¯å“ªé‡Œ", f"**{province}**"),
        ("è¯æ˜æ—¥æœŸæ˜¯ä»€ä¹ˆ", f"**{proof_date}**"),
        ("å§“åæ˜¯ä»€ä¹ˆ", f"**{name}**"),
        ("è¯ä»¶å·ç æ˜¯å¤šå°‘", f"**{id_number}**"),
        ("å•ä½åç§°æ˜¯ä»€ä¹ˆ", f"**{company}**")
    ]

    selected_qa = random.sample(question_templates, min(3, len(question_templates)))

    return selected_qa


def create_extraction_qa_json(image_files, csv_df):
    """åˆ›å»ºä¿¡æ¯æå–QA JSONæ–‡ä»¶"""
    qa_list = []

    for image_file in image_files:
        image_filename = image_file
        logger.info(f"ğŸ” å¤„ç†å›¾ç‰‡: {image_file}ï¼Œå¼€å§‹åŒ¹é…CSVä¸­çš„å§“å...")

        matched = False

        for idx, row in csv_df.iterrows():
            name = str(row['å§“å']).strip()
            if not name:
                continue

            if name in image_filename:
                row_data = row.to_dict()
                logger.info(f"âœ… åŒ¹é…æˆåŠŸï¼šå›¾ç‰‡ {image_file} åŒ…å«å§“å {name}")

                qa_pairs = generate_questions_and_answers(row_data)

                messages = []
                for question, answer in qa_pairs:
                    user_message = {
                        "role": "user",
                        "content": f"<image>\n{question}"
                    }
                    messages.append(user_message)

                    assistant_message = {
                        "role": "assistant",
                        "content": answer
                    }
                    messages.append(assistant_message)

                qa_item = {
                    "images": [f"./images/{image_file}"],
                    "messages": messages
                }

                qa_list.append(qa_item)
                logger.info(f"âœ… å·²ä¸ºå›¾ç‰‡ {image_file} ç”Ÿæˆ {len(qa_pairs)} ä¸ªé—®é¢˜-ç­”æ¡ˆå¯¹")
                matched = True
                break

        if not matched:
            logger.warning(f"âš ï¸  æœªæ‰¾åˆ°åŒ…å«å›¾ç‰‡åç§°çš„å§“åæ•°æ®: {image_file}")

    return qa_list


def generate_annotations(qa_type="all", input_dir=INPUT_DIR, output_dir=OUTPUT_DIR):
    """ç”Ÿæˆæ‰€æœ‰ç±»å‹çš„QAå¯¹"""
    # æ£€æŸ¥CSVæ–‡ä»¶
    csv_path = os.path.join(input_dir, CSV_FILE)
    if not os.path.exists(csv_path):
        logger.error(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        logger.info(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

    # è·å–å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
    image_files = get_image_files(input_dir + "/images")
    if not image_files:
        logger.warning(f"âš ï¸  åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        return

    results = {}

    # ç”Ÿæˆåˆ†ç±»QAå¯¹
    if qa_type in ["classification", "all"]:
        logger.info("ğŸ”„ æ­£åœ¨ç”Ÿæˆåˆ†ç±»QAå¯¹...")
        classification_qa = generate_classification_qa(image_files)

        output_json_path = os.path.join(output_dir, QA_CLASSIFICATION_FILE)
        try:
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(classification_qa, f, ensure_ascii=False, indent=4)
            logger.info(f"âœ… åˆ†ç±»QAå¯¹JSONæ–‡ä»¶å·²ä¿å­˜: {output_json_path}")
            logger.info(f"ğŸ“Š å…±ç”Ÿæˆ {len(classification_qa)} ä¸ªåˆ†ç±»QAå¯¹")
            results["classification"] = output_json_path
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†ç±»QA JSONæ–‡ä»¶å¤±è´¥: {e}")

    # ç”Ÿæˆä¿¡æ¯æå–QAå¯¹
    if qa_type in ["extraction", "all"]:
        csv_df = load_csv_data(csv_path)
        if csv_df is None:
            return

        logger.info("ğŸ”„ æ­£åœ¨ç”Ÿæˆä¿¡æ¯æå–QAå¯¹...")
        extraction_qa = create_extraction_qa_json(image_files, csv_df)

        if extraction_qa:
            output_json_path = os.path.join(output_dir, QA_EXTRACTION_FILE)
            try:
                with open(output_json_path, 'w', encoding='utf-8') as f:
                    json.dump(extraction_qa, f, ensure_ascii=False, indent=4)
                logger.info(f"âœ… ä¿¡æ¯æå–QAå¯¹JSONæ–‡ä»¶å·²ä¿å­˜: {output_json_path}")
                logger.info(f"ğŸ“Š å…±ç”Ÿæˆ {len(extraction_qa)} ä¸ªä¿¡æ¯æå–QAå¯¹")
                results["extraction"] = output_json_path
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜ä¿¡æ¯æå–QA JSONæ–‡ä»¶å¤±è´¥: {e}")
        else:
            logger.warning("âš ï¸  æœªç”Ÿæˆä»»ä½•ä¿¡æ¯æå–QAå¯¹ï¼Œè¯·æ£€æŸ¥å›¾ç‰‡åç§°ä¸CSVæ•°æ®çš„åŒ¹é…æƒ…å†µ")

    return
